<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>第四天打卡(超参数选择方法)</title>
      <link href="/2024/04/25/%E7%AC%AC%E5%9B%9B%E5%A4%A9%E6%89%93%E5%8D%A1/"/>
      <url>/2024/04/25/%E7%AC%AC%E5%9B%9B%E5%A4%A9%E6%89%93%E5%8D%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="超参数选择方法"><a href="#超参数选择方法" class="headerlink" title="超参数选择方法"></a>超参数选择方法</h1><h2 id="一-交叉验证"><a href="#一-交叉验证" class="headerlink" title="一.  交叉验证"></a>一.  交叉验证</h2><h3 id="交叉验证的目的"><a href="#交叉验证的目的" class="headerlink" title="交叉验证的目的"></a>交叉验证的目的</h3><ol><li><p>我们在学习过程中可能会遇到选择多种模型来训练，最后我们需要一种评判标准来选择哪种模型比较好，此时我们就需要用到交叉验证。</p></li><li><p>我们确定了某种学习模型之后，但可能有多组参数的选择，具体选择哪一种我们也需要交叉验证。</p></li></ol><h2 id="二-网格搜索"><a href="#二-网格搜索" class="headerlink" title="二.  网格搜索"></a>二.  网格搜索</h2><h3 id="1-网格搜索的定义"><a href="#1-网格搜索的定义" class="headerlink" title="1.  网格搜索的定义"></a>1.  网格搜索的定义</h3><p>网格搜索是一种常见的参数调优方法，它的根本目的是通过遍历不同的超参数组合，达到模型最优化的效果。比如支持向量机中使用rbf核的情况下，C的调优范围设置为[1, 10, 100]，gamma的范围设置为[0.001, 0.0001]，那么超参数就有3 * 2 &#x3D; 6种组合，网格搜索就是遍历这六种组合，找到其中模型表现最好的超参数的值。</p><h3 id="2-sklearn中的网格搜索方法"><a href="#2-sklearn中的网格搜索方法" class="headerlink" title="2.  sklearn中的网格搜索方法"></a>2.  sklearn中的网格搜索方法</h3><p>使用常见的机器学习模型时，我们通常会选用sklearn这个库。</p><p>使用网格搜索时，我们需要先调用GridSearchCV这个类</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV, KFold</span><br></pre></td></tr></table></figure><p>GridSearchCV中的参数如下：</p><ul><li>estimator：可以理解为一个模型框架吧，比如说用支持向量回归就是svr()，多层感知机就是MLPRegressor()，随机森林就是RandomForestRegressor()，当然这里也可以是一些其他的分类器。</li><li>param_grid：这里一般以字典或者列表形式传入，内容是参数名+参数空间，比如 ‘C’: [1,3,5,7,9,15,20,25,30,40,50,100,200]</li><li>cv：交叉验证方法，这里通常选用k折交叉验证</li><li>scoring：测试集中评估交叉验证模型性能的策略</li></ul><h3 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3.  代码实现"></a>3.  代码实现</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.model_selection import train_test_split, GridSearchCV</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class="line"></span><br><span class="line"># 加载iris数据集</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"># 划分数据集为训练集和测试集</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)</span><br><span class="line"></span><br><span class="line"># 数据标准化</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"># 定义KNN模型</span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line"></span><br><span class="line"># 定义要搜索的参数网格</span><br><span class="line">param_grid = &#123;</span><br><span class="line">    &#x27;n_neighbors&#x27;: range(1, 11),</span><br><span class="line">    &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;],</span><br><span class="line">    &#x27;p&#x27;: [1, 2]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 创建GridSearchCV对象</span><br><span class="line">grid_search = GridSearchCV(knn, param_grid, cv=5, scoring=&#x27;accuracy&#x27;)</span><br><span class="line"></span><br><span class="line"># 拟合GridSearchCV对象</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 打印最佳参数</span><br><span class="line">print(&quot;Best parameters found: &quot;, grid_search.best_params_)</span><br><span class="line"></span><br><span class="line"># 使用最佳参数的模型进行预测</span><br><span class="line">best_knn = grid_search.best_estimator_</span><br><span class="line">y_pred = best_knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"># 打印分类报告和混淆矩阵</span><br><span class="line">print(&quot;Classification report for best model:\n&quot;, classification_report(y_test, y_pred))</span><br><span class="line">print(&quot;Confusion matrix for best model:\n&quot;, confusion_matrix(y_test, y_pred))</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\ProgramData\anaconda3\python.exe C:\Users\LENOVO\PycharmProjects\pythonProject\5.py </span><br><span class="line">Best parameters found:  &#123;&#x27;n_neighbors&#x27;: 6, &#x27;p&#x27;: 2, &#x27;weights&#x27;: &#x27;uniform&#x27;&#125;</span><br><span class="line">Classification report for best model:</span><br><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00        13</span><br><span class="line">           1       0.89      1.00      0.94        16</span><br><span class="line">           2       1.00      0.88      0.93        16</span><br><span class="line"></span><br><span class="line">    accuracy                           0.96        45</span><br><span class="line">   macro avg       0.96      0.96      0.96        45</span><br><span class="line">weighted avg       0.96      0.96      0.96        45</span><br><span class="line"></span><br><span class="line">Confusion matrix for best model:</span><br><span class="line"> [[13  0  0]</span><br><span class="line"> [ 0 16  0]</span><br><span class="line"> [ 0  2 14]]</span><br><span class="line"></span><br><span class="line">进程已结束，退出代码为 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以上便是我对网格搜索和交叉验证的一些理解</p><h2 id="三-利用KNN算法实现手写数字识别"><a href="#三-利用KNN算法实现手写数字识别" class="headerlink" title="三.  利用KNN算法实现手写数字识别"></a>三.  利用KNN算法实现手写数字识别</h2><h3 id="1-步骤"><a href="#1-步骤" class="headerlink" title="1.  步骤"></a>1.  步骤</h3><ol><li><p><strong>数据准备</strong>：首先，你需要一个手写数字的数据集，如MNIST。MNIST数据集包含了大量的手写数字图像（28x28像素）以及对应的标签（0-9）。</p></li><li><p><strong>数据预处理</strong>：将图像数据转化为适合机器学习算法处理的形式。这通常包括缩放、归一化、二值化等步骤。对于MNIST数据集，每个像素值都在0-255之间，你可以将这些值归一化到0-1之间。</p></li><li><p><strong>划分数据集</strong>：将数据集划分为训练集和测试集。训练集用于训练模型，测试集用于评估模型的性能。 </p></li><li><p><strong>特征提取</strong>：从图像中提取特征。对于手写数字识别，通常可以直接使用图像的像素值作为特征。你也可以尝试使用更复杂的特征提取方法，如HOG、LBP等，但这可能会增加计算的复杂性。 </p></li><li><p><strong>训练KNN模型</strong>：使用训练集和提取的特征训练KNN模型。你需要选择一个合适的k值。k值的选择会影响模型的性能。k值太小可能会导致模型过拟合，k值太大可能会导致模型欠拟合。 </p></li><li><p><strong>测试KNN模型</strong>：使用测试集和提取的特征测试KNN模型的性能。你可以计算模型的准确率、召回率、F1值等指标来评估模型的性能。 </p></li><li><p><strong>预测</strong>：对于新的手写数字图像，你可以按照上述步骤提取特征，然后使用训练好的KNN模型进行预测。</p></li></ol><h3 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2.  代码实现"></a>2.  代码实现</h3><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class="line"></span><br><span class="line"># 加载MNIST数据集</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line"></span><br><span class="line"># 显示一些样本图像</span><br><span class="line">for index, (image, label) in enumerate(zip(digits.images[:4], digits.target[:4])):</span><br><span class="line">    plt.subplot(2, 2, index + 1)</span><br><span class="line">    plt.axis(&#x27;off&#x27;)</span><br><span class="line">    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=&#x27;nearest&#x27;)</span><br><span class="line">    plt.title(f&#x27;Label: &#123;label&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># 划分数据集为训练集和测试集</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)</span><br><span class="line"></span><br><span class="line"># 数据预处理：标准化特征</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"># 选择K值并创建KNN分类器</span><br><span class="line">k = 3  # 你可以尝试不同的k值来找到最佳性能</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line"></span><br><span class="line"># 训练KNN模型</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># 在测试集上进行预测</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"># 评估模型性能</span><br><span class="line">print(&quot;Confusion Matrix:&quot;)</span><br><span class="line">print(confusion_matrix(y_test, y_pred))</span><br><span class="line">print(&quot;\nClassification Report:&quot;)</span><br><span class="line">print(classification_report(y_test, y_pred))</span><br><span class="line"></span><br><span class="line"># 显示一些预测结果</span><br><span class="line">for index, (image, prediction) in enumerate(zip(digits.images[-4:], y_pred[-4:])):</span><br><span class="line">    plt.subplot(2, 2, index + 1)</span><br><span class="line">    plt.axis(&#x27;off&#x27;)</span><br><span class="line">    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=&#x27;nearest&#x27;)</span><br><span class="line">    plt.title(f&#x27;Prediction: &#123;prediction&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下：<strong>（运行结果截图导入失败，十分抱歉）</strong></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\ProgramData\anaconda3\python.exe C:\Users\LENOVO\PycharmProjects\pythonProject\6.py </span><br><span class="line">Confusion Matrix:</span><br><span class="line">[[33  0  0  0  0  0  0  0  0  0]</span><br><span class="line"> [ 0 28  0  0  0  0  0  0  0  0]</span><br><span class="line"> [ 0  1 32  0  0  0  0  0  0  0]</span><br><span class="line"> [ 0  0  1 33  0  0  0  0  0  0]</span><br><span class="line"> [ 0  0  0  0 46  0  0  0  0  0]</span><br><span class="line"> [ 0  0  0  0  0 45  1  0  0  1]</span><br><span class="line"> [ 0  0  0  0  0  0 35  0  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0 33  0  1]</span><br><span class="line"> [ 0  1  1  0  0  0  0  0 28  0]</span><br><span class="line"> [ 0  0  0  1  1  1  0  0  1 36]]</span><br><span class="line"></span><br><span class="line">Classification Report:</span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00        33</span><br><span class="line">           1       0.93      1.00      0.97        28</span><br><span class="line">           2       0.94      0.97      0.96        33</span><br><span class="line">           3       0.97      0.97      0.97        34</span><br><span class="line">           4       0.98      1.00      0.99        46</span><br><span class="line">           5       0.98      0.96      0.97        47</span><br><span class="line">           6       0.97      1.00      0.99        35</span><br><span class="line">           7       1.00      0.97      0.99        34</span><br><span class="line">           8       0.97      0.93      0.95        30</span><br><span class="line">           9       0.95      0.90      0.92        40</span><br><span class="line"></span><br><span class="line">    accuracy                           0.97       360</span><br><span class="line">   macro avg       0.97      0.97      0.97       360</span><br><span class="line">weighted avg       0.97      0.97      0.97       360</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">进程已结束，退出代码为 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="四-结尾"><a href="#四-结尾" class="headerlink" title="四.  结尾"></a>四.  结尾</h2><p>以上便是我对今天学习内容的理解。</p>]]></content>
      
      
      
        <tags>
            
            <tag> KNN </tag>
            
            <tag> Github </tag>
            
            <tag> 数据 </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三天打卡(数据归一化，数据标准化)</title>
      <link href="/2024/04/24/%E7%AC%AC%E4%B8%89%E5%A4%A9%E6%89%93%E5%8D%A1/"/>
      <url>/2024/04/24/%E7%AC%AC%E4%B8%89%E5%A4%A9%E6%89%93%E5%8D%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="数据归一化，数据标准化"><a href="#数据归一化，数据标准化" class="headerlink" title="数据归一化，数据标准化"></a>数据归一化，数据标准化</h1><h2 id="一-概述"><a href="#一-概述" class="headerlink" title="一.  概述"></a>一.  概述</h2><p>数据的归一化和标准化是特征缩放的方法，是数据预处理的关键步骤。为了消除评价指标之间的量纲影响，需要进行数据归一化&#x2F;标准化处理，从而解决数据指标之间的可比性。经过这些处理后，每个指标都处在同一数量级，适合进行综合对比评价。</p><h2 id="二-归一化和标准化的区别和作用"><a href="#二-归一化和标准化的区别和作用" class="headerlink" title="二.  归一化和标准化的区别和作用"></a>二.  归一化和标准化的区别和作用</h2><h3 id="1-线性归一化："><a href="#1-线性归一化：" class="headerlink" title="1.    线性归一化："></a>1.    线性归一化：</h3><p>线性归一化也被称作最小-最大规范化或者离散标准化，是对原始数据的线性变换，将数据值映射到[0,1]之间。可以用以下公式表示：<br>$$<br>x’ &#x3D; (x-min(x))&#x2F;(max(x)-min(x))<br>$$<br>代码实现如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def MaxMinNormalization(x,Max,Min):</span><br><span class="line">    x = (x - Min) / (Max - Min);</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure><p>适用于数值比较集中的情况。</p><p>还有以下这种代码：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">def dm01_MinMaxScaler():</span><br><span class="line">    data = [[90, 2, 10, 40],</span><br><span class="line">            [60, 4, 15, 45],</span><br><span class="line">            [75, 3, 13, 46]]</span><br><span class="line"># 2. 初始化归一化对象</span><br><span class="line">    transformer = MinMaxScaler()</span><br><span class="line"># 3. 对原始特征进行变换</span><br><span class="line">    data = transformer.fit_transform(data)</span><br><span class="line"># 4. 打印归一化后的结果</span><br><span class="line">    print(data)</span><br><span class="line">#调用函数</span><br><span class="line">dm01_MinMaxScaler()</span><br></pre></td></tr></table></figure><p>运行结果如下：(截图上传在博客无法显示所以我是复制的运行截图)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\ProgramData\anaconda3\python.exe C:\Users\LENOVO\PycharmProjects\pythonProject\3.py </span><br><span class="line">[[1.         0.         0.         0.        ]</span><br><span class="line"> [0.         1.         1.         0.83333333]</span><br><span class="line"> [0.5        0.5        0.6        1.        ]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-Z-score标准化"><a href="#2-Z-score标准化" class="headerlink" title="2.  Z-score标准化"></a>2.  Z-score标准化</h3><p>标准化是依照特征矩阵的列处理数据。数据标准化方法有多种，如：直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。不同的标准化方法，对系统的评价结果会产生不同的影响。其中，最常用的是Z-Score 标准化。</p><p>公式表示如下：<br>$$<br>x’ &#x3D; (x-mean)&#x2F;μ<br>$$<br><strong>作用：</strong></p><ul><li>提升模型的收敛速度（加快梯度下降的求解速度）</li><li>提升模型的精度（消除量级和量纲的影响）</li><li>简化计算（与归一化的简化原理相同</li></ul><p>代码实现如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def Z_ScoreNormalization(x,mu,sigma):</span><br><span class="line">    x = (x - mu) / sigma;</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure><p>还有如下代码：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dm03_StandardScaler():</span><br><span class="line">    data = [[90, 2, 10, 40],</span><br><span class="line">            [60, 4, 15, 45],</span><br><span class="line">            [75, 3, 13, 46]]</span><br><span class="line"></span><br><span class="line">    # 2. 初始化标准化对象</span><br><span class="line">    transformer = StandardScaler()</span><br><span class="line"></span><br><span class="line">    # 3. 对原始特征进行变换</span><br><span class="line">    data = transformer.fit_transform(data)</span><br><span class="line"></span><br><span class="line">    # 4. 打印归一化后的结果</span><br><span class="line">    print(data)</span><br><span class="line"></span><br><span class="line">    # 5. 打印每列数据的均值和标准差</span><br><span class="line">    print(&#x27;transformer.mean_--&gt;&#x27;, transformer.mean_)</span><br><span class="line">    print(&#x27;transformer.var_--&gt;&#x27;, transformer.var_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 调用函数</span><br><span class="line">dm03_StandardScaler()</span><br></pre></td></tr></table></figure><p>运行结果如下：(截图上传在博客上面没法显示所以我是复制的运行结果)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\ProgramData\anaconda3\python.exe C:\Users\LENOVO\PycharmProjects\pythonProject\0.py </span><br><span class="line">[[ 1.22474487 -1.22474487 -1.29777137 -1.3970014 ]</span><br><span class="line"> [-1.22474487  1.22474487  1.13554995  0.50800051]</span><br><span class="line"> [ 0.          0.          0.16222142  0.88900089]]</span><br><span class="line">transformer.mean_--&gt; [75.          3.         12.66666667 43.66666667]</span><br><span class="line">transformer.var_--&gt; [150.           0.66666667   4.22222222   6.88888889]</span><br><span class="line"></span><br><span class="line">进程已结束，退出代码为 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="三-结束"><a href="#三-结束" class="headerlink" title="三.  结束"></a>三.  结束</h2><p>以上是我今天所学的总结出来的内容。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> Git </tag>
            
            <tag> Github </tag>
            
            <tag> 数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二天打卡（KNN算法）</title>
      <link href="/2024/04/23/%E7%AC%AC%E4%BA%8C%E5%A4%A9%E6%89%93%E5%8D%A1/"/>
      <url>/2024/04/23/%E7%AC%AC%E4%BA%8C%E5%A4%A9%E6%89%93%E5%8D%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h1><h2 id="1-什么是KNN算法？"><a href="#1-什么是KNN算法？" class="headerlink" title="1.什么是KNN算法？"></a>1.什么是KNN算法？</h2><p>KNN 可以说是最简单的分类算法之一，同时，它也是最常用的分类算法之一。注意：KNN 算法是有监督学习中的分类算法，它看起来和另一个机器学习算法 K-means 有点像（K-means 是无监督学习算法），但却是有本质区别的。</p><h2 id="2-算法特点"><a href="#2-算法特点" class="headerlink" title="2.算法特点"></a>2.算法特点</h2><p> KNN是一种非参的、惰性的算法模型。</p><p>(1) 非参的意思是意味着这个模型不会对数据做出任何假设，与其相对的是线性回归。</p><p>(2) 惰性的意思是意味着KNN算法不需要先对数据进行大量训练，它没有明确的训练数据的过程，或者说这个过程非常快。</p><h2 id="3-算法实现"><a href="#3-算法实现" class="headerlink" title="3.算法实现"></a>3.算法实现</h2><h3 id="一-Sklearn-KNN-参数概述"><a href="#一-Sklearn-KNN-参数概述" class="headerlink" title="一.Sklearn KNN 参数概述"></a>一.Sklearn KNN 参数概述</h3><p>要使用 Sklearn KNN 算法进行分类，我们需要先了解 Sklearn KNN 算法的一些基本参数：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def KNeighborsClassifier(n_neighbors = 5,</span><br><span class="line">                       weights=&#x27;uniform&#x27;,</span><br><span class="line">                       algorithm = &#x27;&#x27;,</span><br><span class="line">                       leaf_size = &#x27;30&#x27;,</span><br><span class="line">                       p = 2,</span><br><span class="line">                       metric = &#x27;minkowski&#x27;,</span><br><span class="line">                       metric_params = None,</span><br><span class="line">                       n_jobs = None</span><br><span class="line">                       )</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier  #导入所需要用到的包</span><br><span class="line">#创建模型</span><br><span class="line">x = [[0],[1],[2],[3]]   </span><br><span class="line">y = [0,0,1,1]</span><br><span class="line">model = KNeighborsClassifier(n_neighbors=3)</span><br><span class="line">model.fit(x,y)</span><br><span class="line">mypre = model.predict([[4]])</span><br><span class="line">#输出预测值</span><br><span class="line">print(f&#x27;预测值:&#123;mypre&#125;&#x27;)</span><br></pre></td></tr></table></figure><p><img src="C:\Users\LENOVO\OneDrive\桌面\微信图片_20240423222824.png" alt="微信图片_20240423222824"></p><p>其中：</p><ul><li>n_neighbors：这个值就是指 KNN 中的 “K”了。前面说到过，通过调整 K 值，算法会有不同的效果。</li><li>weights（权重）：最普遍的 KNN 算法无论距离如何，权重都一样，但有时候我们想搞点特殊化，比如距离更近的点让它更加重要。这时候就需要 weight 这个参数了，这个参数有三个可选参数的值，决定了如何分配权重。参数选项如下：</li></ul><p>* ‘uniform’：不管远近权重都一样，就是最普通的 KNN 算法的形式。</p><p>* ‘distance’：权重和距离成反比，距离预测目标越近具有越高的权重。</p><p>* 自定义函数：自定义一个函数，根据输入的坐标值返回对应的权重，达到自</p><p>定义权重的目的。</p><ul><li>algorithm：在 Sklearn 中，要构建 KNN 模型有三种构建方式：</li></ul><p>\1. 暴力法，就是直接计算距离存储比较的那种方式。</p><p>\2. 使用 Kd 树构建 KNN 模型。</p><p>\3. 使用球树构建。</p><p>其中暴力法适合数据较小的方式，否则效率会比较低。如果数据量比较大一般会选择用 Kd 树构建 KNN 模型，而当 Kd 树也比较慢的时候，则可以试试球树来构建 KNN。参数选项如下：</p><p>* ‘brute’ ：蛮力实现；</p><p>* ‘kd_tree’：KD 树实现 KNN；</p><p>* ‘ball_tree’：球树实现 KNN ；</p><p>* ‘auto’： 默认参数，自动选择合适的方法构建模型。</p><p>不过当数据较小或比较稀疏时，无论选择哪个最后都会使用 ‘brute’。</p><p>1 .leaf_size：如果是选择蛮力实现，那么这个值是可以忽略的。当使用 Kd 树或球树，它就是停止建子树的叶子节点数量的阈值。默认30，但如果数据量增多这个参数需要增大，否则速度过慢不说，还容易过拟合。</p><p>2 .p：和 metric 结合使用，当 metric 参数是 “minkowski” 的时候，p&#x3D;1 为曼哈顿距离， p&#x3D;2 为欧式距离。默认为p&#x3D;2。</p><p>3 .metric：指定距离度量方法，一般都是使用欧式距离。</p><p>* ‘euclidean’ ：欧式距离；</p><p>* ‘manhattan’：曼哈顿距离；</p><p>* ‘chebyshev’：切比雪夫距离；</p><p>* ‘minkowski’： 闵可夫斯基距离，默认参数。</p><p>4 .n_jobs：指定多少个CPU进行运算，默认是-1，也就是全部都算。</p><h3 id="二-KNN代码实例"><a href="#二-KNN代码实例" class="headerlink" title="二.  KNN代码实例"></a>二.  KNN代码实例</h3><p>这里采用的是用KNN算法对鸢尾花数据集进行分类。（此代码内容摘抄自CSDN网站）</p><p>在使用 KNN 算法之前，我们要先决定 K 的值是多少。要选出最优的 K 值，可以使用 Sklearn 中的交叉验证方法，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection  import cross_val_score</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">#读取鸢尾花数据集</span><br><span class="line">iris = load_iris()</span><br><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">k_range = range(1, 31)</span><br><span class="line">k_error = []</span><br><span class="line">#循环，取k=1到k=31，查看误差效果</span><br><span class="line">for k in k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    #cv参数决定数据集划分比例，这里是按照5:1划分训练集和测试集</span><br><span class="line">    scores = cross_val_score(knn, x, y, cv=6, scoring=&#x27;accuracy&#x27;)</span><br><span class="line">    k_error.append(1 - scores.mean())</span><br><span class="line"></span><br><span class="line">#画图，x轴为k值，y值为误差值</span><br><span class="line">plt.plot(k_range, k_error)</span><br><span class="line">plt.xlabel(&#x27;Value of K for KNN&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Error&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行后，我们可以得到下面这样的图：</p><p><a href="https://pic1.zhimg.com/v2-ff8325de50e642a2dd23bf28530c3b38_r.jpg">https://pic1.zhimg.com/v2-ff8325de50e642a2dd23bf28530c3b38_r.jpg</a></p><h2 id="4-结束"><a href="#4-结束" class="headerlink" title="4. 结束"></a>4. 结束</h2><p>以上便是我对今天所讲KNN算法的一些理解，剩下的等我再学习一下再进行总结。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> KNN </tag>
            
            <tag> Git </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/04/22/hello-world/"/>
      <url>/2024/04/22/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://allie.github.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">文章摘要:</span><br><span class="line">本文主要介绍了机器学习的一些基础内容。</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">正文：</span><br><span class="line">机器学习简介</span><br><span class="line">1.机器学习背景</span><br><span class="line">伴随着计算机计算能力的不断提升以及大数据时代的迅发展人工智能也取得了前所未有的进步。</span><br><span class="line"></span><br><span class="line">很多企业均开始使用机器学习的相关技术于大部分行业中，以此获得更为强大的洞察力，也为企业的日常生活和企业运营带来了很大的帮助，从而提高了整个产品的服务质量。</span><br><span class="line"></span><br><span class="line">机器学习的典型应用领域有：搜索引擎、自动驾驶、量化投资、计算机视觉、信用卡欺诈检测、游戏、数据挖掘、电子商务、图像识别、自然语言处理、医学诊断、证券金融市场分析以及机器人等相关领域，故在一定程度上，机器学习相关技术的进步也提升了人工智能领域发展的速度。</span><br><span class="line"></span><br><span class="line">2.机器学习简介</span><br><span class="line">机器学习(MachineLearning)，作为计算机科学的子领域，是人工智能领域的重要分支和实现方式。</span><br><span class="line"></span><br><span class="line">机器学习的思想：计算机程序随着经验的积累，能够实现性能的提高。对于某一类任务T及其性能度量P，若一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么就称这个计算机程序在从经验E学习。</span><br><span class="line"></span><br><span class="line">主要的基础理论：数理统计，数学分析，概率论，线性代数，优化理论，数值逼近、计算复杂性理论。</span><br><span class="line"></span><br><span class="line">机器学习的核心元素：算法、数据以及模型。</span><br><span class="line"></span><br><span class="line">3.机器学习简史</span><br><span class="line">作为一门不断发展的学科，机器学习尽管在最近几年才发展成为一门独立的学科。</span><br><span class="line"></span><br><span class="line">起源于20世纪50年代以来人工智能的逻辑推理、启发式搜索、专家系统、符号演算、自动机模型、模糊数学以及神经网络的反向传播BP算法等。如今作为机器学习重要的基础理论。</span><br><span class="line">在1950年代，已经有了机器学习的相关研究。代表工作主要是F.Rosenblatt基于神经感觉科学提出的计算机神经网络，即感知器。随后十年，用于浅层学习的神经网络风靡一时，尤其是MarvinMinsky提出了著名的XOR问题和感知器线性度不可分割的问题。</span><br><span class="line"></span><br><span class="line">局限：由于计算机的计算能力有限，因此很难训练多层网络。通常使用仅具有一个隐藏层的浅层模型。尽管已经陆续提出了各种浅层机器学习模型，但理论分析和应用方面都已产生。但是，理论分析和训练方法的难度要求大量的经验和技能。而随着最近邻算法和其他算法的相继提出，在模型理解，准确性和模型训练方面已经超越了浅层模型。机器学习的发展几乎停滞不前。</span><br><span class="line"></span><br><span class="line">在2006年，希尔顿（Hinton）发表了一篇关于深度信念网络的论文，Bengio等人发表了关于“深度网络的贪婪分层明智训练”的论文，而LeCun团队发表了基于能量模型的“稀疏表示的有效学习”。</span><br><span class="line"></span><br><span class="line">这些事件标志着人工智能正式进入深度网络的实践阶段。同时，云计算和GPU并行计算为深度学习的发展提供了基本保证，尤其是近年来，机器学习它在各个领域都实现了快速发展。新的机器学习算法面临的主要问题更加复杂。机器学习的应用领域已从广度发展到深度，这对模型的训练和应用提出了更高的要求。</span><br><span class="line"></span><br><span class="line">随着人工智能的发展，冯·诺依曼有限状态机的理论基础变得越来越难以满足当前神经网络中层数的要求。这些都给机器学习带来了挑战。</span><br><span class="line">2.什么是机器学习？</span><br><span class="line">监督学习 supervised learning;</span><br><span class="line">非监督学习 unsupervised learning;</span><br><span class="line">半监督学习 semi-supervised learning;</span><br><span class="line">强化学习 reinforcement learning;</span><br><span class="line">监督学习是不断向计算机提供数据（特征），并告诉计算机对应的值（标签），最后通过大量的数据，让计算机自己学会判断和识别。例如Google Photo，你在APP中输入狗，这时就会弹出与狗相关的图片，这背后使用的算法就是监督学习。通过告诉计算机狗长什么样（特征），最后教会计算机认识狗（标签）。再比如今日头条，你使用APP的时间越长，给你推送的内容就越是你平时感兴趣的内容。通过对用户平时日常使用数据（特征）的分析，找到用户的兴趣爱好（标签），进而更精准的推送内容。</span><br><span class="line"></span><br><span class="line">非监督学习与监督学习的区别是，只向计算机提供数据（特征），但并不提供对应的值（标签）。例如需要计算机学会识别猫和狗，这时仅提供猫和狗的图片（特征），但是并不告诉计算机，哪些图片是猫，哪些图片是狗，让计算机自己去总结归纳出两者的特征规律。</span><br><span class="line"></span><br><span class="line">半监督学习是综合了监督学习和非监督学习两者的特点，利用少量有标签的样本，和大量没有标签的样本对计算机进行训练。</span><br><span class="line"></span><br><span class="line">强化学习是将计算机放入一个陌生的环境中，让它自己去学习，其中包含了4个关键要素，分别是环境（environment）、状态（state）、行动（action）和奖励（reward）。例如要设计一款自动投篮机器，首先让机器自己去选择投篮的角度、力度等动作进行尝试，告诉机器如果投篮命中便能获得奖励，之后机器会根据练习所产生的数据，不断修改自身的动作策略，经过数次迭代之后，学习并完成投篮任务。战胜李世石的AlphaGo，所使用的就是强化学习。强化学习与监督学习和非监督学习最大的不同是，不需要使用海量的数据去“喂养”机器，而是通过不断地尝试去获取数据。</span><br><span class="line"></span><br><span class="line">使用Python进行机器学习时，都会用到一个非常强大的第三方包，那就是scikit-learn。</span><br><span class="line"></span><br><span class="line">Sklearn包含了四类算法，分别是回归（regression）、分类（classification）、聚类（clustering）和降维（dimensionality reduction）。其中回归和分类是监督式学习，下面使用Python对简单线性回归和逻辑回归分类进行简要介绍。</span><br><span class="line">以上是我今天想要介绍的东西，谢谢大家欣赏！</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
