<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>第二天打卡（KNN算法）</title>
      <link href="/2024/04/23/%E7%AC%AC%E4%BA%8C%E5%A4%A9%E6%89%93%E5%8D%A1/"/>
      <url>/2024/04/23/%E7%AC%AC%E4%BA%8C%E5%A4%A9%E6%89%93%E5%8D%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h1><h2 id="1-什么是KNN算法？"><a href="#1-什么是KNN算法？" class="headerlink" title="1.什么是KNN算法？"></a>1.什么是KNN算法？</h2><p>KNN 可以说是最简单的分类算法之一，同时，它也是最常用的分类算法之一。注意：KNN 算法是有监督学习中的分类算法，它看起来和另一个机器学习算法 K-means 有点像（K-means 是无监督学习算法），但却是有本质区别的。</p><h2 id="2-算法特点"><a href="#2-算法特点" class="headerlink" title="2.算法特点"></a>2.算法特点</h2><p> KNN是一种非参的、惰性的算法模型。</p><p>(1) 非参的意思是意味着这个模型不会对数据做出任何假设，与其相对的是线性回归。</p><p>(2) 惰性的意思是意味着KNN算法不需要先对数据进行大量训练，它没有明确的训练数据的过程，或者说这个过程非常快。</p><h2 id="3-算法实现"><a href="#3-算法实现" class="headerlink" title="3.算法实现"></a>3.算法实现</h2><h3 id="一-Sklearn-KNN-参数概述"><a href="#一-Sklearn-KNN-参数概述" class="headerlink" title="一.Sklearn KNN 参数概述"></a>一.Sklearn KNN 参数概述</h3><p>要使用 Sklearn KNN 算法进行分类，我们需要先了解 Sklearn KNN 算法的一些基本参数：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def KNeighborsClassifier(n_neighbors = 5,</span><br><span class="line">                       weights=&#x27;uniform&#x27;,</span><br><span class="line">                       algorithm = &#x27;&#x27;,</span><br><span class="line">                       leaf_size = &#x27;30&#x27;,</span><br><span class="line">                       p = 2,</span><br><span class="line">                       metric = &#x27;minkowski&#x27;,</span><br><span class="line">                       metric_params = None,</span><br><span class="line">                       n_jobs = None</span><br><span class="line">                       )</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>n_neighbors：这个值就是指 KNN 中的 “K”了。前面说到过，通过调整 K 值，算法会有不同的效果。</li><li>weights（权重）：最普遍的 KNN 算法无论距离如何，权重都一样，但有时候我们想搞点特殊化，比如距离更近的点让它更加重要。这时候就需要 weight 这个参数了，这个参数有三个可选参数的值，决定了如何分配权重。参数选项如下：</li></ul><p>* ‘uniform’：不管远近权重都一样，就是最普通的 KNN 算法的形式。</p><p>* ‘distance’：权重和距离成反比，距离预测目标越近具有越高的权重。</p><p>* 自定义函数：自定义一个函数，根据输入的坐标值返回对应的权重，达到自</p><p>定义权重的目的。</p><ul><li>algorithm：在 Sklearn 中，要构建 KNN 模型有三种构建方式：</li></ul><p>\1. 暴力法，就是直接计算距离存储比较的那种方式。</p><p>\2. 使用 Kd 树构建 KNN 模型。</p><p>\3. 使用球树构建。</p><p>其中暴力法适合数据较小的方式，否则效率会比较低。如果数据量比较大一般会选择用 Kd 树构建 KNN 模型，而当 Kd 树也比较慢的时候，则可以试试球树来构建 KNN。参数选项如下：</p><p>* ‘brute’ ：蛮力实现；</p><p>* ‘kd_tree’：KD 树实现 KNN；</p><p>* ‘ball_tree’：球树实现 KNN ；</p><p>* ‘auto’： 默认参数，自动选择合适的方法构建模型。</p><p>不过当数据较小或比较稀疏时，无论选择哪个最后都会使用 ‘brute’。</p><p>1 .leaf_size：如果是选择蛮力实现，那么这个值是可以忽略的。当使用 Kd 树或球树，它就是停止建子树的叶子节点数量的阈值。默认30，但如果数据量增多这个参数需要增大，否则速度过慢不说，还容易过拟合。</p><p>2 .p：和 metric 结合使用，当 metric 参数是 “minkowski” 的时候，p&#x3D;1 为曼哈顿距离， p&#x3D;2 为欧式距离。默认为p&#x3D;2。</p><p>3 .metric：指定距离度量方法，一般都是使用欧式距离。</p><p>* ‘euclidean’ ：欧式距离；</p><p>* ‘manhattan’：曼哈顿距离；</p><p>* ‘chebyshev’：切比雪夫距离；</p><p>* ‘minkowski’： 闵可夫斯基距离，默认参数。</p><p>4 .n_jobs：指定多少个CPU进行运算，默认是-1，也就是全部都算。</p><h3 id="二-KNN代码实例"><a href="#二-KNN代码实例" class="headerlink" title="二.  KNN代码实例"></a>二.  KNN代码实例</h3><p>这里采用的是用KNN算法对鸢尾花数据集进行分类。（此代码内容摘抄自CSDN网站）</p><p>在使用 KNN 算法之前，我们要先决定 K 的值是多少。要选出最优的 K 值，可以使用 Sklearn 中的交叉验证方法，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection  import cross_val_score</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">#读取鸢尾花数据集</span><br><span class="line">iris = load_iris()</span><br><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">k_range = range(1, 31)</span><br><span class="line">k_error = []</span><br><span class="line">#循环，取k=1到k=31，查看误差效果</span><br><span class="line">for k in k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    #cv参数决定数据集划分比例，这里是按照5:1划分训练集和测试集</span><br><span class="line">    scores = cross_val_score(knn, x, y, cv=6, scoring=&#x27;accuracy&#x27;)</span><br><span class="line">    k_error.append(1 - scores.mean())</span><br><span class="line"></span><br><span class="line">#画图，x轴为k值，y值为误差值</span><br><span class="line">plt.plot(k_range, k_error)</span><br><span class="line">plt.xlabel(&#x27;Value of K for KNN&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Error&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行后，我们可以得到下面这样的图：</p><p><a href="https://pic1.zhimg.com/v2-ff8325de50e642a2dd23bf28530c3b38_r.jpg">https://pic1.zhimg.com/v2-ff8325de50e642a2dd23bf28530c3b38_r.jpg</a></p><h2 id="4-结束"><a href="#4-结束" class="headerlink" title="4. 结束"></a>4. 结束</h2><p>以上便是我对今天所讲KNN算法的一些理解，剩下的等我再学习一下再进行总结。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/04/22/hello-world/"/>
      <url>/2024/04/22/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://allie.github.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">文章摘要:</span><br><span class="line">本文主要介绍了机器学习的一些基础内容。</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">正文：</span><br><span class="line">机器学习简介</span><br><span class="line">1.机器学习背景</span><br><span class="line">伴随着计算机计算能力的不断提升以及大数据时代的迅发展人工智能也取得了前所未有的进步。</span><br><span class="line"></span><br><span class="line">很多企业均开始使用机器学习的相关技术于大部分行业中，以此获得更为强大的洞察力，也为企业的日常生活和企业运营带来了很大的帮助，从而提高了整个产品的服务质量。</span><br><span class="line"></span><br><span class="line">机器学习的典型应用领域有：搜索引擎、自动驾驶、量化投资、计算机视觉、信用卡欺诈检测、游戏、数据挖掘、电子商务、图像识别、自然语言处理、医学诊断、证券金融市场分析以及机器人等相关领域，故在一定程度上，机器学习相关技术的进步也提升了人工智能领域发展的速度。</span><br><span class="line"></span><br><span class="line">2.机器学习简介</span><br><span class="line">机器学习(MachineLearning)，作为计算机科学的子领域，是人工智能领域的重要分支和实现方式。</span><br><span class="line"></span><br><span class="line">机器学习的思想：计算机程序随着经验的积累，能够实现性能的提高。对于某一类任务T及其性能度量P，若一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么就称这个计算机程序在从经验E学习。</span><br><span class="line"></span><br><span class="line">主要的基础理论：数理统计，数学分析，概率论，线性代数，优化理论，数值逼近、计算复杂性理论。</span><br><span class="line"></span><br><span class="line">机器学习的核心元素：算法、数据以及模型。</span><br><span class="line"></span><br><span class="line">3.机器学习简史</span><br><span class="line">作为一门不断发展的学科，机器学习尽管在最近几年才发展成为一门独立的学科。</span><br><span class="line"></span><br><span class="line">起源于20世纪50年代以来人工智能的逻辑推理、启发式搜索、专家系统、符号演算、自动机模型、模糊数学以及神经网络的反向传播BP算法等。如今作为机器学习重要的基础理论。</span><br><span class="line">在1950年代，已经有了机器学习的相关研究。代表工作主要是F.Rosenblatt基于神经感觉科学提出的计算机神经网络，即感知器。随后十年，用于浅层学习的神经网络风靡一时，尤其是MarvinMinsky提出了著名的XOR问题和感知器线性度不可分割的问题。</span><br><span class="line"></span><br><span class="line">局限：由于计算机的计算能力有限，因此很难训练多层网络。通常使用仅具有一个隐藏层的浅层模型。尽管已经陆续提出了各种浅层机器学习模型，但理论分析和应用方面都已产生。但是，理论分析和训练方法的难度要求大量的经验和技能。而随着最近邻算法和其他算法的相继提出，在模型理解，准确性和模型训练方面已经超越了浅层模型。机器学习的发展几乎停滞不前。</span><br><span class="line"></span><br><span class="line">在2006年，希尔顿（Hinton）发表了一篇关于深度信念网络的论文，Bengio等人发表了关于“深度网络的贪婪分层明智训练”的论文，而LeCun团队发表了基于能量模型的“稀疏表示的有效学习”。</span><br><span class="line"></span><br><span class="line">这些事件标志着人工智能正式进入深度网络的实践阶段。同时，云计算和GPU并行计算为深度学习的发展提供了基本保证，尤其是近年来，机器学习它在各个领域都实现了快速发展。新的机器学习算法面临的主要问题更加复杂。机器学习的应用领域已从广度发展到深度，这对模型的训练和应用提出了更高的要求。</span><br><span class="line"></span><br><span class="line">随着人工智能的发展，冯·诺依曼有限状态机的理论基础变得越来越难以满足当前神经网络中层数的要求。这些都给机器学习带来了挑战。</span><br><span class="line">2.什么是机器学习？</span><br><span class="line">监督学习 supervised learning;</span><br><span class="line">非监督学习 unsupervised learning;</span><br><span class="line">半监督学习 semi-supervised learning;</span><br><span class="line">强化学习 reinforcement learning;</span><br><span class="line">监督学习是不断向计算机提供数据（特征），并告诉计算机对应的值（标签），最后通过大量的数据，让计算机自己学会判断和识别。例如Google Photo，你在APP中输入狗，这时就会弹出与狗相关的图片，这背后使用的算法就是监督学习。通过告诉计算机狗长什么样（特征），最后教会计算机认识狗（标签）。再比如今日头条，你使用APP的时间越长，给你推送的内容就越是你平时感兴趣的内容。通过对用户平时日常使用数据（特征）的分析，找到用户的兴趣爱好（标签），进而更精准的推送内容。</span><br><span class="line"></span><br><span class="line">非监督学习与监督学习的区别是，只向计算机提供数据（特征），但并不提供对应的值（标签）。例如需要计算机学会识别猫和狗，这时仅提供猫和狗的图片（特征），但是并不告诉计算机，哪些图片是猫，哪些图片是狗，让计算机自己去总结归纳出两者的特征规律。</span><br><span class="line"></span><br><span class="line">半监督学习是综合了监督学习和非监督学习两者的特点，利用少量有标签的样本，和大量没有标签的样本对计算机进行训练。</span><br><span class="line"></span><br><span class="line">强化学习是将计算机放入一个陌生的环境中，让它自己去学习，其中包含了4个关键要素，分别是环境（environment）、状态（state）、行动（action）和奖励（reward）。例如要设计一款自动投篮机器，首先让机器自己去选择投篮的角度、力度等动作进行尝试，告诉机器如果投篮命中便能获得奖励，之后机器会根据练习所产生的数据，不断修改自身的动作策略，经过数次迭代之后，学习并完成投篮任务。战胜李世石的AlphaGo，所使用的就是强化学习。强化学习与监督学习和非监督学习最大的不同是，不需要使用海量的数据去“喂养”机器，而是通过不断地尝试去获取数据。</span><br><span class="line"></span><br><span class="line">使用Python进行机器学习时，都会用到一个非常强大的第三方包，那就是scikit-learn。</span><br><span class="line"></span><br><span class="line">Sklearn包含了四类算法，分别是回归（regression）、分类（classification）、聚类（clustering）和降维（dimensionality reduction）。其中回归和分类是监督式学习，下面使用Python对简单线性回归和逻辑回归分类进行简要介绍。</span><br><span class="line">以上是我今天想要介绍的东西，谢谢大家欣赏！</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
